---
# Prometheus Alerting Rules for SaaSOdoo Platform
#
# These rules define conditions that trigger alerts.
# Alerts are sent to Alertmanager for routing and notification.
#
# Alert Severity Levels:
# - critical: Immediate action required (paging)
# - warning: Attention needed soon (email/slack)
# - info: For awareness only
#

---
# SaaSOdoo Application Alerts
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: saasodoo-application-alerts
  namespace: monitoring
  labels:
    app.kubernetes.io/name: prometheus
    app.kubernetes.io/component: alerting
    app.kubernetes.io/part-of: saasodoo
    prometheus: saasodoo
spec:
  groups:
    # Service Health Alerts
    - name: saasodoo.service.health
      interval: 30s
      rules:
        # Service Down
        - alert: ServiceDown
          expr: up{job=~"user-service|billing-service|instance-service|notification-service|database-service"} == 0
          for: 1m
          labels:
            severity: critical
            service: "{{ $labels.job }}"
          annotations:
            summary: "Service {{ $labels.job }} is down"
            description: "Service {{ $labels.job }} has been down for more than 1 minute."
            runbook_url: "https://docs.saasodoo.com/runbooks/service-down"

        # High Error Rate
        - alert: HighErrorRate
          expr: |
            (
              sum(rate(http_requests_total{status=~"5.."}[5m])) by (job)
              /
              sum(rate(http_requests_total[5m])) by (job)
            ) > 0.05
          for: 5m
          labels:
            severity: warning
            service: "{{ $labels.job }}"
          annotations:
            summary: "High error rate on {{ $labels.job }}"
            description: "{{ $labels.job }} has {{ $value | humanizePercentage }} error rate over last 5 minutes."

        # High Latency
        - alert: HighLatency
          expr: |
            histogram_quantile(0.95,
              sum(rate(http_request_duration_seconds_bucket[5m])) by (le, job)
            ) > 1
          for: 5m
          labels:
            severity: warning
            service: "{{ $labels.job }}"
          annotations:
            summary: "High latency on {{ $labels.job }}"
            description: "P95 latency is {{ $value }}s on {{ $labels.job }}."

        # Too Many Restarts
        - alert: PodRestartingTooOften
          expr: increase(kube_pod_container_status_restarts_total{namespace="saasodoo"}[1h]) > 3
          for: 10m
          labels:
            severity: warning
            service: "{{ $labels.container }}"
          annotations:
            summary: "Pod {{ $labels.pod }} restarting too often"
            description: "Pod {{ $labels.pod }} has restarted {{ $value }} times in the last hour."

    # Instance Provisioning Alerts
    - name: saasodoo.instance.provisioning
      interval: 30s
      rules:
        # Instance Provisioning Failed
        - alert: InstanceProvisioningFailed
          expr: |
            increase(celery_task_failed_total{task_name=~".*provision.*"}[15m]) > 0
          for: 5m
          labels:
            severity: critical
            service: instance-service
          annotations:
            summary: "Instance provisioning failures detected"
            description: "{{ $value }} instance provisioning tasks have failed in the last 15 minutes."

        # Instance Provisioning Queue Backlog
        - alert: InstanceProvisioningBacklog
          expr: |
            rabbitmq_queue_messages{queue="instance_provisioning"} > 10
          for: 15m
          labels:
            severity: warning
            service: instance-service
          annotations:
            summary: "Instance provisioning queue has backlog"
            description: "{{ $value }} messages waiting in provisioning queue for >15 minutes."

        # Celery Worker Down
        - alert: CeleryWorkerDown
          expr: |
            up{job="instance-worker"} == 0
          for: 2m
          labels:
            severity: critical
            service: instance-worker
          annotations:
            summary: "Celery worker is down"
            description: "Instance worker has been down for more than 2 minutes."

---
# PostgreSQL/CloudNativePG Alerts
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: saasodoo-postgresql-alerts
  namespace: monitoring
  labels:
    app.kubernetes.io/name: prometheus
    app.kubernetes.io/component: alerting
    app.kubernetes.io/part-of: saasodoo
    prometheus: saasodoo
spec:
  groups:
    - name: saasodoo.postgresql
      interval: 30s
      rules:
        # PostgreSQL Instance Down
        - alert: PostgreSQLDown
          expr: cnpg_collector_up == 0
          for: 1m
          labels:
            severity: critical
            service: postgresql
          annotations:
            summary: "PostgreSQL instance {{ $labels.cluster }} is down"
            description: "PostgreSQL cluster {{ $labels.cluster }} has been down for more than 1 minute."

        # PostgreSQL Replication Lag
        - alert: PostgreSQLReplicationLag
          expr: cnpg_pg_replication_lag_seconds > 30
          for: 5m
          labels:
            severity: warning
            service: postgresql
          annotations:
            summary: "PostgreSQL replication lag on {{ $labels.cluster }}"
            description: "Replication lag is {{ $value }}s on cluster {{ $labels.cluster }}."

        # PostgreSQL High Connections
        - alert: PostgreSQLHighConnections
          expr: |
            (
              cnpg_pg_stat_activity_count / cnpg_pg_settings_max_connections
            ) > 0.8
          for: 5m
          labels:
            severity: warning
            service: postgresql
          annotations:
            summary: "PostgreSQL connection pool near capacity"
            description: "{{ $value | humanizePercentage }} of max connections used on {{ $labels.cluster }}."

        # PostgreSQL Deadlocks
        - alert: PostgreSQLDeadlocks
          expr: increase(cnpg_pg_stat_database_deadlocks_total[5m]) > 0
          for: 1m
          labels:
            severity: warning
            service: postgresql
          annotations:
            summary: "PostgreSQL deadlocks detected"
            description: "{{ $value }} deadlocks detected in the last 5 minutes on {{ $labels.datname }}."

        # PostgreSQL Disk Space
        - alert: PostgreSQLDiskSpaceLow
          expr: |
            (
              1 - (cnpg_pg_database_size_bytes / cnpg_pg_settings_max_wal_size_bytes)
            ) < 0.2
          for: 5m
          labels:
            severity: warning
            service: postgresql
          annotations:
            summary: "PostgreSQL disk space low"
            description: "PostgreSQL cluster {{ $labels.cluster }} is running low on disk space."

---
# RabbitMQ Alerts
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: saasodoo-rabbitmq-alerts
  namespace: monitoring
  labels:
    app.kubernetes.io/name: prometheus
    app.kubernetes.io/component: alerting
    app.kubernetes.io/part-of: saasodoo
    prometheus: saasodoo
spec:
  groups:
    - name: saasodoo.rabbitmq
      interval: 30s
      rules:
        # RabbitMQ Node Down
        - alert: RabbitMQNodeDown
          expr: rabbitmq_identity_info == 0
          for: 1m
          labels:
            severity: critical
            service: rabbitmq
          annotations:
            summary: "RabbitMQ node is down"
            description: "RabbitMQ node {{ $labels.pod }} has been down for more than 1 minute."

        # RabbitMQ Queue Messages High
        - alert: RabbitMQQueueMessagesHigh
          expr: rabbitmq_queue_messages > 1000
          for: 10m
          labels:
            severity: warning
            service: rabbitmq
          annotations:
            summary: "RabbitMQ queue {{ $labels.queue }} has high message count"
            description: "Queue {{ $labels.queue }} has {{ $value }} messages waiting."

        # RabbitMQ Memory High
        - alert: RabbitMQMemoryHigh
          expr: |
            (
              rabbitmq_process_resident_memory_bytes / rabbitmq_resident_memory_limit_bytes
            ) > 0.8
          for: 5m
          labels:
            severity: warning
            service: rabbitmq
          annotations:
            summary: "RabbitMQ memory usage high"
            description: "RabbitMQ is using {{ $value | humanizePercentage }} of memory limit."

        # RabbitMQ Disk Free Low
        - alert: RabbitMQDiskFreeLow
          expr: rabbitmq_disk_space_available_bytes < 5368709120  # 5GB
          for: 5m
          labels:
            severity: warning
            service: rabbitmq
          annotations:
            summary: "RabbitMQ disk space low"
            description: "RabbitMQ has only {{ $value | humanize1024 }}B disk space available."

---
# Kubernetes Infrastructure Alerts
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: saasodoo-kubernetes-alerts
  namespace: monitoring
  labels:
    app.kubernetes.io/name: prometheus
    app.kubernetes.io/component: alerting
    app.kubernetes.io/part-of: saasodoo
    prometheus: saasodoo
spec:
  groups:
    - name: saasodoo.kubernetes
      interval: 30s
      rules:
        # Node Not Ready
        - alert: NodeNotReady
          expr: kube_node_status_condition{condition="Ready",status="true"} == 0
          for: 5m
          labels:
            severity: critical
            service: kubernetes
          annotations:
            summary: "Kubernetes node {{ $labels.node }} is not ready"
            description: "Node {{ $labels.node }} has been in NotReady state for more than 5 minutes."

        # Pod CrashLooping
        - alert: PodCrashLooping
          expr: |
            increase(kube_pod_container_status_restarts_total{namespace="saasodoo"}[15m]) > 0
            and
            kube_pod_container_status_waiting_reason{reason="CrashLoopBackOff"} == 1
          for: 5m
          labels:
            severity: critical
            service: "{{ $labels.container }}"
          annotations:
            summary: "Pod {{ $labels.pod }} is crash looping"
            description: "Pod {{ $labels.pod }} in namespace {{ $labels.namespace }} is in CrashLoopBackOff."

        # PVC Almost Full
        - alert: PVCAlmostFull
          expr: |
            (
              kubelet_volume_stats_used_bytes / kubelet_volume_stats_capacity_bytes
            ) > 0.85
          for: 5m
          labels:
            severity: warning
            service: storage
          annotations:
            summary: "PVC {{ $labels.persistentvolumeclaim }} is almost full"
            description: "PVC {{ $labels.persistentvolumeclaim }} is {{ $value | humanizePercentage }} full."

        # High CPU Usage
        - alert: HighCPUUsage
          expr: |
            (
              sum(rate(container_cpu_usage_seconds_total{namespace="saasodoo"}[5m])) by (pod)
              /
              sum(kube_pod_container_resource_limits{resource="cpu",namespace="saasodoo"}) by (pod)
            ) > 0.9
          for: 10m
          labels:
            severity: warning
            service: "{{ $labels.pod }}"
          annotations:
            summary: "High CPU usage on {{ $labels.pod }}"
            description: "Pod {{ $labels.pod }} is using {{ $value | humanizePercentage }} of CPU limit."

        # High Memory Usage
        - alert: HighMemoryUsage
          expr: |
            (
              sum(container_memory_working_set_bytes{namespace="saasodoo"}) by (pod)
              /
              sum(kube_pod_container_resource_limits{resource="memory",namespace="saasodoo"}) by (pod)
            ) > 0.9
          for: 10m
          labels:
            severity: warning
            service: "{{ $labels.pod }}"
          annotations:
            summary: "High memory usage on {{ $labels.pod }}"
            description: "Pod {{ $labels.pod }} is using {{ $value | humanizePercentage }} of memory limit."
